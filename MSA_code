
# https://www.ffiec.gov/Medianincome.htm (MSA) 

# https://freddiemac.embs.com/FLoan/Data/download3.php (FMac)

# https://www.census.gov/data/tables/time-series/demo/popest/2010s-total-metro-and-micro-statistical-areas.html#par_textimage (POP) 

# https://www.census.gov/library/publications/2012/dec/c2010sr-01.html (Race) 

library(magrittr)
library(readxl)
library(caret)
library(tidyverse)
library(keras)
library(tensorflow)

# Advantages of Loan dataset
# 1. big data 80+ GB
# 2. use of models to predict delic
# 3. group by state to create delic index (compare with survey)
# 4. use in predicting home prices, plug into lstm? or just VAR
# 5. 




# Data --------------------------------------------------------------------

orig_2006 <- read_delim("C:/Users/Bruger/Desktop/FreddieMac_data_M4/sample_2006/sample_orig_2006.txt", 
                        "|", escape_double = FALSE, col_names = FALSE, 
                        trim_ws = TRUE, col_types = cols(X26 = col_skip()))

orig_2006 %>% rename(id                = X20,
                     credit_score      = X1,
                     date              = X2,
                     maturity          = X4,
                     new_homeowner     = X3,
                     area              = X5,
                     insurance_pct     = X6,
                     units             = X7,
                     ocu_status        = X8,
                     com_loan_to_value = X9,
                     debt_to_income    = X10,
                     upb               = X11,
                     loan_to_value     = X12,
                     rate              = X13,
                     channel           = X14,
                     ppm               = X15,
                     loan_type         = X16,
                     state             = X17,
                     property_type     = X18,
                     postal_code       = X19,
                     loan_purpose      = X21,
                     term              = X22,
                     n_borrowers       = X23,
                     seller            = X24,
                     servicer          = X25) -> orig_2006



svcg_2006 <- read_delim("C:/Users/Bruger/Desktop/FreddieMac_data_M4/sample_2006/sample_svcg_2006.txt", 
                        "|", escape_double = FALSE, col_names = FALSE, 
                        trim_ws = TRUE, col_types = cols_only(X1 = col_guess(),
                                                              X2 = col_guess(),
                                                              X3 = col_guess(),
                                                              X4 = col_guess()))


svcg_2006 %>% rename(id                = X1,
                     date_info         = X2,
                     unpaid            = X3,
                     delic             = X4) -> svcg_2006


orig_2006 <- orig_2006 %>% select(id, date, everything()) %>% 
  mutate(date     = as.Date(paste0(as.character(date),"01"), format = "%Y%m%d"),
         maturity = as.Date(paste0(as.character(maturity),"01"), format = "%Y%m%d"))



svcg_2006 <- svcg_2006 %>% 
  mutate(date_info = as.Date(paste0(as.character(date_info),"01"), format = "%Y%m%d"))


# Combination -------------------------------------------------------------

svcg_2006$temp <- if_else(svcg_2006$delic == 1, svcg_2006$date_info, as.Date(NA))

extra <- svcg_2006 %>% 
  group_by(id) %>% 
  mutate(prev = lag(delic, order_by=id),
         survived = ifelse(prev != 0 & delic == 0, 1, 0)) %>% 
  summarize(delic_mean   = mean(delic, na.rm=T),
            delic_binary = mean(delic, na.rm=T) > 0,
            delic_date   = first(na.omit(temp)),
            surv_binary  = mean(survived, na.rm=T) > 0,
            surv         = sum(survived, na.rm=T),
            max_unpaid   = max(unpaid, na.rm=T),
            recovered    = last(delic) == 0) %>% 
  mutate(first_complete_stop = ifelse(delic_binary == TRUE & surv_binary == FALSE, TRUE, FALSE),
         recovered           = ifelse(delic_binary == TRUE & recovered == TRUE, TRUE, FALSE))


data <- left_join(orig_2006, extra, by = "id")



# exploration -------------------------------------------------------------

data %>% count(state, sort = T)

data %>% group_by(delic_binary) %>%  count(seller, sort = T)

data %>% group_by(delic_date, state) %>% summarize(n = sum(delic_binary)) %>% 
  ggplot(aes(delic_date, n))  + geom_line() + facet_wrap(~state, scales="free")



# Extra data -------------------------------------------------------------


Inc_MSA <- read_excel("C:/Users/Bruger/Desktop/FreddieMac_data_M4/MedianIncome_MSA.xls", 
                       skip = 1) %>% 
rename( area = `MSA/MD FIPS CODE NO.` ,
        median_income = `2018 FFIEC EST. MSA/MD MEDIAN FAMILY INCOME**`) %>%  select(c(1,4))

Race_MSA <- read_excel("C:/Users/Bruger/Desktop/FreddieMac_data_M4/Race_MSA.xlsx") %>% 
  select(-c(2,12)) %>% na.omit(area)

Race_MSA$area %<>% as.numeric("area")



data_f <- data %>% 
  left_join(Inc_MSA, by = "area") %>% 
  left_join(Race_MSA, by = "area")





# additional exploration -----------------------------------------------------------

u <- unique(Race_MSA$area)
length(u)

data %>% count(area) %>% arrange(desc(n))
data_f %>% count(median_income) %>% arrange(desc(n))
data_f %>% count(White) %>% arrange(desc(n))
Race_MSA %>% count(area) %>% arrange(desc(n))


# ML ----------------------------------------------------------------------

ctrl <- trainControl(method = "cv", 
                     number = 10)

index    <- createDataPartition(data_f$delic_binary, p = 0.75, list = FALSE)
training <- data_f[index,] 
test     <- data_f[-index,] 


fit_ela <- train(price     ~ .,
                 data      = training,
                 trControl = cv, 
                 tuneGrid  = expand.grid(alpha  = seq(0, 1,    by = 0.1),
                                         lambda = seq(1, 1000, by = 100)),
                 method    = "glmnet", 
                 family    = "gaussian")

model <- keras_model_sequential() %>% 
  layer_dense(input_shape = dim(training)[2], units = 128, activation = "relu") %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dropout(0.3) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(1, activation = "sigmoid")
